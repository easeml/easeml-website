---
layout: list
title: CI
aliases: [/ci]
summary: Machine learning models are software artefacts. Among the stream of models generated by ease.ml/AutoML, not all of them satisfy the based requirement of real-​world deployment. Can we continuously test ML models in the way we are testing traditional softwares?
thumbnail: images/easeml-component-generic.png
weight: 6
sections:
    - partial: content
    - title: Publications
      partial: list
      content:
        data: publications
        where:
          key: tag
          operator: eq
          match: ci
      params:
        header:
          field: year
        grouped: true
links:
  - icon: fa-github
    href: https://github.com/easeml/ci
    large: true
    label: Code
---

<div class="embedded-element">
{{< youtube m8aV1HnOnN0 >}}
</div>

Ease.ml/ci is a continuous integration engine developed for ML. Given a new machine learning model committed into the system, and a set of user-​specified conditions and test dataset (e.g., the new model is at least 1 points better than the old model), ease.ml/ci checks whether the given model satisfies all the test conditions.

One technical challenge is overfitting — after every test query, the test set will lose some of its statistical power. If we are not being careful, after a while, we are going to overfit to the provided test set and ease.ml/ci would potentially return a wrong answer. The technical core of ease.ml/ci is a collection of techniques to measure the “information leakage” coming along with each test query, and inform the user when a new test dataset is required.

**Input:** (1) An endless stream of models trained by the AutoML system; (2) A test set and a list of test conditions.

**Output:** An endless stream of models, each of which is labelled by {Pass, Failure}.

**Action:** The user has to provide a new test set when ease.ml/ci requests so.
